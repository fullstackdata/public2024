# !pip install  llama-index-vector-stores-milvus llama-index-llms-huggingface llama-index-embeddings-huggingface sentence-transformers
# !pip install -U transformers accelerate bitsandbytes

# Create an index over the documents
from llama_index.core import VectorStoreIndex, StorageContext, load_index_from_storage
from llama_index.vector_stores.milvus import MilvusVectorStore

vector_store = MilvusVectorStore(
    uri="./milvus_llamaindex.db", dim=384, overwrite=False
)
storage_context = StorageContext.from_defaults(vector_store=vector_store)

import logging
import sys

logging.basicConfig(stream=sys.stdout, level=logging.INFO)
logging.getLogger().addHandler(logging.StreamHandler(stream=sys.stdout)) 

from llama_index.core import VectorStoreIndex, StorageContext
from llama_index.core.schema import TextNode



from transformers import AutoTokenizer, AutoModelForCausalLM, BitsAndBytesConfig

quantization_config = BitsAndBytesConfig(load_in_8bit=True)
# model_name = "NousResearch/DeepHermes-3-Llama-3-8B-Preview"
model_name = "microsoft/phi-4"
model = AutoModelForCausalLM.from_pretrained(
    model_name, 
    device_map="auto",
    quantization_config=quantization_config
)

tokenizer = AutoTokenizer.from_pretrained(model_name, device_map='auto' )

from llama_index.core.llms import ChatMessage
from llama_index.llms.huggingface import HuggingFaceLLM

llm = HuggingFaceLLM(tokenizer=tokenizer, model=model, device_map='auto')

from llama_index.core import Settings
Settings.llm = llm

from llama_index.embeddings.huggingface import HuggingFaceEmbedding

embed_model = HuggingFaceEmbedding(model_name="BAAI/bge-small-en-v1.5")
Settings.embed_model = embed_model

index = VectorStoreIndex(nodes, storage_context=storage_context)


from llama_index.core.retrievers import VectorIndexAutoRetriever
from llama_index.core.vector_stores.types import MetadataInfo, VectorStoreInfo


vector_store_info = VectorStoreInfo(
    content_info="brief biography of celebrities",
    metadata_info=[
        MetadataInfo(
            name="category",
            type="str",
            description=(
                "Category of the celebrity, one of [Sports, Entertainment,"
                " Business, Music]"
            ),
        ),
        MetadataInfo(
            name="country",
            type="str",
            description=(
                "Country of the celebrity, one of [United States, Barbados,"
                " Portugal]"
            ),
        ),
    ],
)
retriever = VectorIndexAutoRetriever(
    index, vector_store_info=vector_store_info, verbose=True
)

retriever.retrieve("Tell me about two sports celebrities from United States")
