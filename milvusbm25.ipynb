{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "machine_shape": "hm",
      "gpuType": "A100",
      "authorship_tag": "ABX9TyNDuQwP6BPFsiiXCyAd/Fg3",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/fullstackdata/public2024/blob/main/milvusbm25.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nrzCTfeWBHpS",
        "outputId": "409e3449-bd3f-4251-a49b-c15cea138451"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting sqlite4\n",
            "  Downloading SQLite4-0.1.1-py3-none-any.whl.metadata (5.8 kB)\n",
            "Downloading SQLite4-0.1.1-py3-none-any.whl (7.1 kB)\n",
            "Installing collected packages: sqlite4\n",
            "Successfully installed sqlite4-0.1.1\n"
          ]
        }
      ],
      "source": [
        "# !pip install -U pymilvus\n",
        "# !pip install -U transformers accelerate bitsandbytes\n",
        "# !pip install  llama-index-vector-stores-milvus llama-index-llms-huggingface llama-index-embeddings-huggingface sentence-transformers einops\n",
        "# !pip install -U llama-index-vector-stores-milvus wikipedia-api\n",
        "# !pip install -U FlagEmbedding\n",
        "# !pip install -U datasets fsspec\n",
        "!pip install -U sqlite4\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from pymilvus import MilvusClient\n",
        "import os\n",
        "\n",
        "# Try a new unique name for the database file\n",
        "db_file_name = \"milvus_demo_new3.db\"\n",
        "\n",
        "# Optional: Clean up if it exists from a previous failed run\n",
        "if os.path.exists(db_file_name):\n",
        "    os.remove(db_file_name)\n",
        "    print(f\"Removed existing {db_file_name}\")\n",
        "\n",
        "try:\n",
        "    client = MilvusClient(db_file_name)\n",
        "    print(f\"Successfully started Milvus Lite with database: {db_file_name}\")\n",
        "\n",
        "    # You can now proceed with your Milvus operations\n",
        "    # Example:\n",
        "    # client.create_collection(\n",
        "    #     collection_name=\"my_collection\",\n",
        "    #     dimension=128,\n",
        "    #     metric_type=\"L2\",\n",
        "    #     # Add a primary field and vector field if needed\n",
        "    # )\n",
        "    # print(\"Collection created.\")\n",
        "\n",
        "except Exception as e:\n",
        "    print(f\"Error starting Milvus Lite: {e}\")\n",
        "    # You can print more detailed traceback if needed:\n",
        "    # import traceback\n",
        "    # traceback.print_exc()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qpxrFAcYKCiC",
        "outputId": "a994dd9f-d006-4a36-a73e-067e4488268f"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "ERROR:root:Start milvus-lite failed\n",
            "2025-06-25 13:01:16,357 [ERROR][_create_connection]: Failed to create new connection using: 9fa22253208e4ad7a09a6aec9176ac37 (milvus_client.py:916)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Error starting Milvus Lite: <ConnectionConfigException: (code=1, message=Open local milvus failed)>\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Create an index over the documents\n",
        "from llama_index.core import VectorStoreIndex, StorageContext, load_index_from_storage\n",
        "from llama_index.vector_stores.milvus import MilvusVectorStore\n",
        "\n",
        "from llama_index.vector_stores.milvus.utils import BM25BuiltInFunction\n",
        "from llama_index.core import VectorStoreIndex, StorageContext, load_index_from_storage\n",
        "from llama_index.vector_stores.milvus import MilvusVectorStore\n",
        "\n",
        "bm25_function = BM25BuiltInFunction(\n",
        "    analyzer_params={\n",
        "        \"tokenizer\": \"standard\",\n",
        "        \"filter\": [\n",
        "            \"lowercase\",  # Built-in filter\n",
        "            {\"type\": \"length\", \"max\": 40},  # Custom cap size of a single token\n",
        "            {\"type\": \"stop\", \"stop_words\": [\"of\", \"to\"]},  # Custom stopwords\n",
        "        ],\n",
        "    },\n",
        "    enable_match=True,\n",
        ")\n",
        "\n",
        "\n",
        "from transformers import AutoTokenizer, AutoModelForCausalLM, BitsAndBytesConfig\n",
        "\n",
        "quantization_config = BitsAndBytesConfig(load_in_8bit=True)\n",
        "model_name = \"microsoft/phi-4\"\n",
        "model = AutoModelForCausalLM.from_pretrained(\n",
        "    model_name,\n",
        "    device_map=\"auto\",\n",
        "    quantization_config=quantization_config\n",
        ")\n",
        "\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_name, device_map='auto' )\n",
        "\n",
        "from llama_index.core.llms import ChatMessage\n",
        "from llama_index.llms.huggingface import HuggingFaceLLM\n",
        "\n",
        "llm = HuggingFaceLLM(tokenizer=tokenizer, model=model, device_map='auto')\n",
        "\n",
        "from llama_index.core import Settings\n",
        "Settings.llm = llm\n",
        "\n",
        "from llama_index.embeddings.huggingface import HuggingFaceEmbedding\n",
        "\n",
        "embed_model = HuggingFaceEmbedding(model_name=\"nomic-ai/nomic-embed-text-v1.5\", trust_remote_code=True, device='cuda')\n",
        "Settings.embed_model = embed_model"
      ],
      "metadata": {
        "id": "N38WERrCBNDe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# # prompt: use hugging face datasets to download argilla/news dataset and save it as jsonl file\n",
        "\n",
        "# # !pip install datasets\n",
        "# from datasets import load_dataset\n",
        "# import json\n",
        "\n",
        "# # Load the dataset\n",
        "# dataset = load_dataset(\"argilla/news\")\n",
        "\n",
        "# # Save the dataset to a JSONL file\n",
        "# with open(\"argilla_news.jsonl\", \"w\") as f:\n",
        "#   for item in dataset[\"train\"]:\n",
        "#     f.write(json.dumps(item) + \"\\n\")\n",
        "\n",
        "# print(\"Dataset saved to argilla_news.jsonl\")"
      ],
      "metadata": {
        "id": "AVE-5yiLByhU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from llama_index.core import Document\n",
        "import json\n",
        "\n",
        "documents = []\n",
        "with open('/content/argilla_news.jsonl', 'r') as f:\n",
        "    data = [json.loads(line) for line in f]\n",
        "\n",
        "for itm in data:\n",
        "  documents.append(Document(text=itm[\"text\"]))\n",
        "\n",
        "\n",
        "from llama_index.vector_stores.milvus import MilvusVectorStore\n",
        "from llama_index.core import VectorStoreIndex\n",
        "from llama_index.core import StorageContext\n",
        "from llama_index.vector_stores.milvus.utils import BGEM3SparseEmbeddingFunction\n",
        "\n",
        "# # bm25_function needs to be defined or imported if used\n",
        "# # Assuming bm25_function is defined elsewhere or is a placeholder\n",
        "# bm25_function = None # Replace with actual BM25 function if needed\n",
        "\n",
        "fields = [\n",
        "    {\"name\": \"embedding\", \"type\": \"FLOAT_VECTOR\", \"dim\": 768}, # Assuming your dense embeddings have dimension 768\n",
        "    {\"name\": \"text\", \"type\": \"VARCHAR\", \"max_length\": 65535},\n",
        "    {\"name\": \"sparse_embedding\", \"type\": \"SPARSE_FLOAT_VECTOR\"}, # Add the sparse embedding field\n",
        "]\n",
        "\n",
        "vector_store = MilvusVectorStore(\n",
        "    \"./milvus_llamaindex.db\",\n",
        "    dim=768,\n",
        "    # enable_sparse=True,\n",
        "    # sparse_embedding_function=BGEM3SparseEmbeddingFunction(),\n",
        "    # fields=fields,\n",
        "    # overwrite=True,\n",
        "    # drop_old=True,\n",
        ")\n",
        "\n",
        "storage_context = StorageContext.from_defaults(vector_store=vector_store)\n",
        "index = VectorStoreIndex.from_documents(documents, storage_context=storage_context, llm=llm)\n",
        "\n",
        "from llama_index.core.prompts import RichPromptTemplate\n",
        "from llama_index.core import PromptTemplate\n",
        "\n",
        "template = \"\"\"\n",
        "You are a Q&A chat bot.\n",
        "Use the given context only, answer the question.\n",
        "If you don't know the answer, say that you don't know the answer.\n",
        "\n",
        "<context>\n",
        "{context_str}\n",
        "</context>\n",
        "\n",
        "Question: {query_str}\n",
        "\"\"\"\n",
        "\n",
        "prompt_template = PromptTemplate(template=template)\n",
        "qe = index.as_query_engine(embed_model=embed_model, similarity_top_k=10)\n",
        "qe.update_prompts({\"response_synthesizer:text_qa_template\": prompt_template})\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "bvBETbvzBShg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "response = qe.query(\"What are some of the films mentioned?\")\n",
        "print(response)\n",
        "\n"
      ],
      "metadata": {
        "id": "IyOXVYNOBbrP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for nd in response.source_nodes:\n",
        "  print(\"***************\")\n",
        "  print(nd.node.text)"
      ],
      "metadata": {
        "id": "-_UvN7CZBe99"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}